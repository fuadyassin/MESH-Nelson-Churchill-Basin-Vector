{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b880f9-e8f2-49e4-a721-ed8d5fefa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ddb      = '/home/fuaday/scratch/ncrb-models/MESH-ncrb/MESH_drainage_database.nc'\n",
    "db = xs.open_dataset(input_ddb)\n",
    "lon = db.variables['lon'].values\n",
    "lat = db.variables['lat'].values\n",
    "segid = db.variables['subbasin'].values\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b8d2a-1570-4a44-a1e0-261c2736aea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths and directories\n",
    "directory = '/scratch/fuaday/ncrb-models/soil-texture'\n",
    "input_basin    = '/home/fuaday/scratch/ncrb-models/geofabric-outputs/ncrb-geofabric/ncrb_subbasins.shp'\n",
    "input_ddb      = '/home/fuaday/scratch/ncrb-models/MESH-ncrb/MESH_drainage_database.nc'\n",
    "OUTPUT_SHAPEFILE = 'merged_soil_data_shapefile4.shp'\n",
    "merged_gsde_soil_data = 'merged_gsde_soil_data.csv'\n",
    "file_names = [\n",
    "    'st_stats_CLAY1.csv', 'st_stats_CLAY2.csv',\n",
    "    'st_stats_SAND1.csv', 'st_stats_SAND2.csv',\n",
    "    'st_stats_BD1.csv', 'st_stats_BD2.csv',\n",
    "    'st_stats_OC1.csv', 'st_stats_OC2.csv'\n",
    "]\n",
    "file_paths = [os.path.join(directory, filename) for filename in file_names]\n",
    "\n",
    "# Load and merge CSV files\n",
    "def load_and_merge_files(file_list, key='COMID'):\n",
    "    dfs = [pd.read_csv(fp) for fp in file_list]\n",
    "    return reduce(lambda left, right: pd.merge(left, right, on=key, how='outer'), dfs)\n",
    "\n",
    "gsde_df = load_and_merge_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f7a21-7998-4cfc-bc70-7c2801b71e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for search and replacement\n",
    "searchname = ['_depth=4.5', '_depth=9.1000004', '_depth=16.6', '_depth=28.9', '_depth=49.299999', '_depth=82.900002', '_depth=138.3', '_depth=229.60001']\n",
    "replacename = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "\n",
    "# Replace column names and remove periods\n",
    "new_columns = []\n",
    "for column in gsde_df.columns:\n",
    "    for search, replace in zip(searchname, replacename):\n",
    "        if search in column:\n",
    "            column = column.replace(search, replace)\n",
    "    column = column.replace('.', '')  # This line removes periods from column names\n",
    "    new_columns.append(column)\n",
    "\n",
    "gsde_df.columns = new_columns\n",
    "\n",
    "#print(gsde_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af44315-31ce-4130-affb-ed6ec91d4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data using forward and backward fill sorted based on COMID\n",
    "# Replace values greater than 100 with NaN, excluding 'COMID' and columns containing 'OC' or 'BD'\n",
    "for col in gsde_df.columns:\n",
    "    if col != 'COMID' and 'OC' not in col and 'BD' not in col:  # Exclude 'COMID', and columns with 'OC' or 'BD'\n",
    "        gsde_df.loc[gsde_df[col] > 100, col] = np.nan\n",
    "\n",
    "# Sort by COMID for logical filling\n",
    "gsde_df.sort_values('COMID', inplace=True)\n",
    "\n",
    "# Fill missing values using forward and backward filling\n",
    "gsde_df.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "gsde_df.fillna(method='bfill', inplace=True)  # Backward fill if any NA still exists after ffill\n",
    "\n",
    "# Optionally, check the result to confirm that no more NAs exist\n",
    "print(gsde_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ffa9a-6393-456c-883f-3090a3aedc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth intervals\n",
    "gsde_intervals = [(0, 0.045), (0.045, 0.091), (0.091, 0.166), (0.166, 0.289), \n",
    "                  (0.289, 0.493), (0.493, 0.829), (0.829, 1.383), (1.383, 2.296)]\n",
    "mesh_intervals = [(0, 0.1), (0.1, 0.35), (0.35, 1.2), (1.2, 4.1)]\n",
    "\n",
    "# Calculate weights\n",
    "def calculate_weights(gsde_intervals, mesh_intervals):\n",
    "    weights_used = []\n",
    "    for mesh_interval in mesh_intervals:\n",
    "        start, end = mesh_interval\n",
    "        weights = []\n",
    "        for gsde_interval in gsde_intervals:\n",
    "            gsde_start, gsde_end = gsde_interval\n",
    "            overlap_start = max(start, gsde_start)\n",
    "            overlap_end = min(end, gsde_end)\n",
    "            weight = (overlap_end - overlap_start) / (end - start) if overlap_start < overlap_end else 0\n",
    "            weights.append(weight)\n",
    "        weights_used.append([w / sum(weights) for w in weights if sum(weights) > 0])\n",
    "    return weights_used\n",
    "\n",
    "weights_used = calculate_weights(gsde_intervals, mesh_intervals)\n",
    "print(weights_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ef31c-9792-40c4-8e0c-e9326fe3cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names for each soil property\n",
    "# properties = ['CLAY', 'SAND', 'BD', 'OC']\n",
    "# depths = ['4.5', '9.1000004', '16.6', '28.9', '49.299999', '82.900002', '138.3', '229.60001']\n",
    "# column_names = {prop: [f'mean.{prop}_depth={depth}' for depth in depths] for prop in properties}\n",
    "properties = ['CLAY', 'SAND', 'BD', 'OC']\n",
    "depths = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "column_names = {prop: [f'mean{prop}{depth}' for depth in depths] for prop in properties}\n",
    "print(column_names)\n",
    "# Extract and calculate weighted sums directly into gsde_df\n",
    "for prop, cols in column_names.items():\n",
    "    #extracted_data = gsde_df[cols].fillna(0)  # Handle missing values\n",
    "    extracted_data = gsde_df[cols]  # Handle missing values\n",
    "    #print(extracted_data)\n",
    "    weights_array = np.array(weights_used).T  # Transpose to align for dot product\n",
    "    print(weights_array)\n",
    "    mesh_values = np.dot(extracted_data, weights_array)\n",
    "    # Apply conversion factor specifically for OC\n",
    "    if prop == 'OC':\n",
    "        mesh_values *= 0.01 * 1.72\n",
    "    for i, mesh_col in enumerate([f'mesh{prop}{j+1}' for j in range(len(mesh_intervals))]):\n",
    "        gsde_df[mesh_col] = mesh_values[:, i]\n",
    "\n",
    "\n",
    "#print(gsde_df[['meshCLAY1', 'meshCLAY2', 'meshCLAY3', 'meshCLAY4']].head()) # Check results\n",
    "gsde_df.to_csv(os.path.join(directory, merged_gsde_soil_data), index=False) # Save DataFrame if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d7846-734c-4c08-80d4-43b2efb9a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsde_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8c8a3-f9b6-447f-bbad-092198f36f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare shapefile\n",
    "gdf = gpd.read_file(input_basin).to_crs(epsg=4326)\n",
    "gdf['COMID'] = gdf['COMID'].astype(int)\n",
    "\n",
    "# Merge DataFrame with GeoDataFrame based on 'COMID'\n",
    "merged_gdf = gdf.merge(gsde_df, on='COMID', how='left')\n",
    "\n",
    "# Save the merged GeoDataFrame to a shapefile\n",
    "output_shapefile = os.path.join(directory, OUTPUT_SHAPEFILE)\n",
    "merged_gdf.to_file(output_shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325a925-6268-4e62-87c3-54710c9e5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(gsde_df[['meshCLAY1', 'meshCLAY2', 'meshCLAY3', 'meshCLAY4']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5109d-c51f-4f67-b5f0-9da90b7e6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "gru_columns = merged_gdf[['meshSAND1', 'meshCLAY1', 'meshSAND4', 'meshCLAY4']]\n",
    "# gru_columns = merged_gdf.columns[-16:-1]  # Excludes the very last column\n",
    "\n",
    "# Create a modified jet colormap that sets zero values to white\n",
    "cmap = plt.cm.gnuplot2_r.copy()\n",
    "cmap.set_under('white', alpha=0)\n",
    "\n",
    "# Set up the figure and axes for a 2x2 grid (4 plots)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "\n",
    "# Create an Image for colormap scaling\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=mcolors.Normalize(vmin=0, vmax=100))\n",
    "sm._A = []  # Fake up the array of the scalar mappable. Urgh...\n",
    "\n",
    "# Loop through and plot each column\n",
    "for i, col in enumerate(gru_columns):\n",
    "    ax = axes.flatten()[i]\n",
    "    merged_gdf.plot(column=col, ax=ax, cmap=cmap, vmin=0, vmax=100)\n",
    "    ax.set_title(col, fontsize=12)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Hide any unused subplots\n",
    "for i in range(len(gru_columns), axes.flatten().shape[0]):\n",
    "    axes.flatten()[i].set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "fig.tight_layout(pad=1.0)\n",
    "fig.subplots_adjust(right=0.85, hspace=0.1, wspace=0.1)\n",
    "\n",
    "# Place colorbar\n",
    "cbar_ax = fig.add_axes([0.87, 0.15, 0.03, 0.7])\n",
    "fig.colorbar(sm, cax=cbar_ax)\n",
    "# Save the figure if needed\n",
    "plt.savefig('soiltexture_plots.png')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105b65f-b6eb-4f80-b436-781f0472f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Extract indices of forcing IDs based on the drainage database\n",
    "ind = []\n",
    "for i in range(len(segid)):\n",
    "    fid = np.where(np.int32(merged_gdf['COMID'].values) == segid[i])[0]\n",
    "    ind = np.append(ind, fid)\n",
    "ind = np.int32(ind)\n",
    "\n",
    "# NetCDF file creation\n",
    "ncname = 'ncrb'\n",
    "ncfname = f\"MESH_parameters_{ncname}.nc\"\n",
    "\n",
    "# Check if the file can be created in the current directory\n",
    "try:\n",
    "    rootgrp = nc.Dataset(ncfname, \"w\", format=\"NETCDF4\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: {ncfname}. Trying to create the file in a temporary directory.\")\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    ncfname = os.path.join(temp_dir, f\"MESH_parameters_{ncname}.nc\")\n",
    "    rootgrp = nc.Dataset(ncfname, \"w\", format=\"NETCDF4\")\n",
    "    print(f\"File created in temporary directory: {ncfname}\")\n",
    "\n",
    "# Define dimensions\n",
    "MaxNumGRUs = 16\n",
    "num_soil_lyrs = 4\n",
    "subbasin_dim = rootgrp.createDimension(\"subbasin\", len(merged_gdf['COMID']))\n",
    "ngru_dim = rootgrp.createDimension(\"ngru\", MaxNumGRUs)\n",
    "nsol_dim = rootgrp.createDimension(\"nsol\", num_soil_lyrs)\n",
    "\n",
    "# Define variables with their respective dimensions\n",
    "lon_var = rootgrp.createVariable(\"lon\", \"f4\", (\"subbasin\",), fill_value=-1.0)\n",
    "lat_var = rootgrp.createVariable(\"lat\", \"f4\", (\"subbasin\",), fill_value=-1.0)\n",
    "clay_var = rootgrp.createVariable(\"clay\", \"f4\", (\"nsol\", \"subbasin\"), fill_value=-1.0)\n",
    "sand_var = rootgrp.createVariable(\"sand\", \"f4\", (\"nsol\", \"subbasin\"), fill_value=-1.0)\n",
    "orgm_var = rootgrp.createVariable(\"orgm\", \"f4\", (\"nsol\", \"subbasin\"), fill_value=-1.0)\n",
    "# Add more variables as needed...\n",
    "\n",
    "# Assign attributes to variables\n",
    "lon_var.units = \"degrees_east\"\n",
    "lat_var.units = \"degrees_north\"\n",
    "clay_var.long_name = \"Clay Content of Soil Layer\"\n",
    "sand_var.long_name = \"Sand Content of Soil Layer\"\n",
    "orgm_var.long_name = \"Organic Matter Content of Soil Layer\"\n",
    "# Add more attributes as needed...\n",
    "\n",
    "# Assign data to variables using indices\n",
    "lon_var[:] = np.array(lon[ind])\n",
    "lat_var[:] = np.array(lat[ind])\n",
    "\n",
    "# Assign data to soil layers\n",
    "clay_var[0, :] = np.array(merged_gdf['meanCLAY1'].values[ind])\n",
    "clay_var[1, :] = np.array(merged_gdf['meanCLAY2'].values[ind])\n",
    "clay_var[2, :] = np.array(merged_gdf['meanCLAY3'].values[ind])\n",
    "clay_var[3, :] = np.array(merged_gdf['meanCLAY4'].values[ind])\n",
    "\n",
    "sand_var[0, :] = np.array(merged_gdf['meanSAND1'].values[ind])\n",
    "sand_var[1, :] = np.array(merged_gdf['meanSAND2'].values[ind])\n",
    "sand_var[2, :] = np.array(merged_gdf['meanSAND3'].values[ind])\n",
    "sand_var[3, :] = np.array(merged_gdf['meanSAND4'].values[ind])\n",
    "\n",
    "orgm_var[0, :] = np.array(merged_gdf['meanOC1'].values[ind])\n",
    "orgm_var[1, :] = np.array(merged_gdf['meanOC2'].values[ind])\n",
    "orgm_var[2, :] = np.array(merged_gdf['meanOC3'].values[ind])\n",
    "orgm_var[3, :] = np.array(merged_gdf['meanOC4'].values[ind])\n",
    "\n",
    "# Assign global attributes\n",
    "rootgrp.Conventions = \"CF-1.0\"\n",
    "rootgrp.source = \"MERIT geogabrics and GSDE soil\"\n",
    "rootgrp.institution = \"ECCC\"\n",
    "rootgrp.references = \"xx et al. (xxxx) journal xx:xx-xx\"\n",
    "rootgrp.history = f\"Fuad Yassin, {datetime.now().strftime('%Y-%m-%d')}\"\n",
    "rootgrp.featureType = \"point\"\n",
    "\n",
    "# Close the file to write data to disk\n",
    "rootgrp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f4659-521d-4263-b700-445b73d3228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract indices of forcing IDs based on the drainage database\n",
    "ind = []\n",
    "for i in range(len(segid)):\n",
    "    fid = np.where(np.int32(merged_gdf['COMID'].values) == segid[i])[0]\n",
    "    ind = np.append(ind, fid)\n",
    "ind = np.int32(ind)\n",
    "\n",
    "# NetCDF file creation\n",
    "ncname = 'ncrb'\n",
    "ncfname = f\"MESH_parameters_{ncname}.nc\"\n",
    "\n",
    "# Check if the file can be created in the current directory\n",
    "try:\n",
    "    rootgrp = nc.Dataset(ncfname, \"w\", format=\"NETCDF4\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: {ncfname}. Trying to create the file in a temporary directory.\")\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    ncfname = os.path.join(temp_dir, f\"MESH_parameters_{ncname}.nc\")\n",
    "    rootgrp = nc.Dataset(ncfname, \"w\", format=\"NETCDF4\")\n",
    "    print(f\"File created in temporary directory: {ncfname}\")\n",
    "\n",
    "# Define dimensions\n",
    "MaxNumGRUs = 16\n",
    "num_soil_lyrs = 4\n",
    "subbasin_dim = rootgrp.createDimension(\"subbasin\", len(merged_gdf['COMID']))\n",
    "ngru_dim = rootgrp.createDimension(\"ngru\", MaxNumGRUs)\n",
    "nsol_dim = rootgrp.createDimension(\"nsol\", num_soil_lyrs)\n",
    "\n",
    "# Define variables with their respective dimensions\n",
    "lon_var = rootgrp.createVariable(\"lon\", \"f4\", (\"subbasin\",), fill_value=-1.0)\n",
    "lat_var = rootgrp.createVariable(\"lat\", \"f4\", (\"subbasin\",), fill_value=-1.0)\n",
    "time_var = rootgrp.createVariable(\"time\", \"f4\", (\"subbasin\",), fill_value=-1.0)\n",
    "clay_var = rootgrp.createVariable(\"clay\", \"f4\", (\"nsol\", \"subbasin\"), fill_value=-1.0)\n",
    "sand_var = rootgrp.createVariable(\"sand\", \"f4\", (\"nsol\", \"subbasin\"), fill_value=-1.0)\n",
    "orgm_var = rootgrp.createVariable(\"orgm\", \"f4\", (\"nsol\", \"subbasin\"), fill_value=-1.0)\n",
    "# Add more variables as needed...\n",
    "\n",
    "# Assign attributes to variables\n",
    "lon_var.units = \"degrees_east\"\n",
    "lat_var.units = \"degrees_north\"\n",
    "time_var.units = \"days since 1980-10-01 00:00:00.0 -0:00\"\n",
    "clay_var.long_name = \"Clay Content of Soil Layer\"\n",
    "sand_var.long_name = \"Sand Content of Soil Layer\"\n",
    "orgm_var.long_name = \"Organic Matter Content of Soil Layer\"\n",
    "# Add more attributes as needed...\n",
    "\n",
    "# Assign data to variables using indices\n",
    "lon_var[:] = np.array(lon[ind])\n",
    "lat_var[:] = np.array(lat[ind])\n",
    "time_var[:] = np.zeros(len(merged_gdf['COMID']))  # Assuming time is set to 0\n",
    "\n",
    "# Assign data to soil layers\n",
    "clay_var[0, :] = np.array(merged_gdf['meanCLAY1'].values[ind])\n",
    "clay_var[1, :] = np.array(merged_gdf['meanCLAY2'].values[ind])\n",
    "clay_var[2, :] = np.array(merged_gdf['meanCLAY3'].values[ind])\n",
    "clay_var[3, :] = np.array(merged_gdf['meanCLAY4'].values[ind])\n",
    "\n",
    "sand_var[0, :] = np.array(merged_gdf['meanSAND1'].values[ind])\n",
    "sand_var[1, :] = np.array(merged_gdf['meanSAND2'].values[ind])\n",
    "sand_var[2, :] = np.array(merged_gdf['meanSAND3'].values[ind])\n",
    "sand_var[3, :] = np.array(merged_gdf['meanSAND4'].values[ind])\n",
    "\n",
    "orgm_var[0, :] = np.array(merged_gdf['meanOC1'].values[ind])\n",
    "orgm_var[1, :] = np.array(merged_gdf['meanOC2'].values[ind])\n",
    "orgm_var[2, :] = np.array(merged_gdf['meanOC3'].values[ind])\n",
    "orgm_var[3, :] = np.array(merged_gdf['meanOC4'].values[ind])\n",
    "\n",
    "# Assign global attributes\n",
    "rootgrp.Conventions = \"CF-1.0\"\n",
    "rootgrp.source = \"MERIT geogabrics and GSDE soil\"\n",
    "rootgrp.institution = \"ECCC\"\n",
    "rootgrp.references = \"xx et al. (xxxx) journal xx:xx-xx\"\n",
    "rootgrp.history = f\"Fuad Yassin, {datetime.now().strftime('%Y-%m-%d')}\"\n",
    "rootgrp.featureType = \"point\"\n",
    "\n",
    "# Close the file to write data to disk\n",
    "rootgrp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d77db-181f-461d-a7a3-36ff3932e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scienv",
   "language": "python",
   "name": "scienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
